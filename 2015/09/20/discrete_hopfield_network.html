<!DOCTYPE html><!--[if lt IE 7]>      <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="description" content="NeuPy is a Python library for Artificial Neural Networks. NeuPy supports many different types of Neural Networks from a simple perceptron to deep learning models.">
        <meta name="viewport" content="width=device-width">
        <title>Discrete Hopfield Network &mdash; NeuPy</title>
            <link rel="stylesheet" href="../../../_static/normalize.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/sphinx.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/main.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/flat.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
            <link rel="stylesheet" href="../../../_static/font-awesome.min.css" type="text/css">
        <link rel="shortcut icon" href="../../../_static/favicon.ico" /><!-- Load modernizr and JQuery -->
        <script src="../../../_static/vendor/modernizr-2.6.2.min.js"></script>
        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"></script>
        <script>window.jQuery || document.write('<script src="../../../_static/vendor/jquery-1.8.2.min.js"><\/script>')</script>
        <script src="../../../_static/plugins.js"></script>
        <script src="../../../_static/main.js"></script>
        <link rel="search" title="Search" href="../../../search.html" /><link rel="next" title="Predict prices for houses in the area of Boston" href="../../07/04/boston_house_prices_dataset.html" /><link rel="prev" title="Password recovery" href="../21/password_recovery.html" /><link rel="alternate" type="application/rss+xml" title="RSS" href="../../../rss.html" /><script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '1.7.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        SOURCELINK_SUFFIX: '.txt',
        HAS_SOURCE:  true
      };
    </script><script type="text/javascript" src="../../../_static/underscore.js"></script><script type="text/javascript" src="../../../_static/doctools.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="../../../_static/disqus.js"></script><script type="text/javascript" src="../../../_static/js/google_analytics.js"></script><script type="text/javascript" src="../../../_static/js/script.js"></script><script type="text/javascript" src="../../../_static/js/copybutton.js"></script>

    <script type="text/javascript">
        $(document).ready(function () {
            // Scroll to content if on small screen
            if (screen.width < 480)
            {
                $(document).scrollTop(document.getElementsByTagName("article")[0].offsetTop - 44);
            }
        });
    </script>
    <style media="screen" type="text/css">
        img[src="../_images/mnist-code-sample-home.png"] { max-width: 600px !important; }
        table { background-color: white; }
        table.docutils.citation  { background-color: inherit; }
        div.highlight { margin-bottom: 20px !important; }

        .limited-width { max-width: 800px; margin: auto; }
        .docutils { width: 100%; }
        a .docutils.literal {
            background-color: inherit !important;
            padding: 0px !important;
            color: #3bbc46 !important;
        }
        .docutils td { padding: 10px; }
        .section { word-wrap:break-word; }
        .descname { font-weight: bold; }
        .highlight-python + .figure { margin-top: 20px; }
        .dataframe { text-align: center !important; width: 100%; margin: 10px 0 10px 0; }
        .dataframe td { padding: 5px; }

        .math .gd { color: #000 !important; } /* Generic.Deleted */
        .math .m { color: #000 !important; } /* Literal.Number */
        .math .s { color: #000 !important; } /* Literal.String */
        .math .mf { color: #000 !important; } /* Literal.Number.Float */
        .math .mh { color: #000 !important; } /* Literal.Number.Hex */
        .math .mi { color: #000 !important; } /* Literal.Number.Integer */
        .math .mo { color: #000 !important; } /* Literal.Number.Oct */
        .math .sc { color: #000 !important; } /* Literal.String.Char */
        .math .s2 { color: #000 !important; } /* Literal.String.Double */
        .math .si { color: #000 !important; } /* Literal.String.Interpol */
        .math .sx { color: #000 !important; } /* Literal.String.Other */
        .math .s1 { color: #000 !important; } /* Literal.String.Single */
        .math .ss { color: #000 !important; } /* Literal.String.Symbol */
        .math .il { color: #000 !important; } /* Literal.Number.Integer.Long */

        /* Background for class and function names */
        dt[id^="neupy."] {
            background-color: #e6edf2;
            border: 1px solid #f8fafb;
            border-radius: 8px;
            padding: 10px 20px;
        }
        div[id^="module-neupy."] h1 {
            display: none;
        }

        /* Search input field */
        .search-input {
            width: 100%;
            padding: 10px;
            display: block;
        }
        .box {
          padding-bottom: 50px;
        }
        .search-input-container {
          width: 100%;
          vertical-align: middle;
          white-space: nowrap;
          position: relative;
        }
        .search-input-container input#search {
          width: 100%;
          height: 50px;
          padding-left: 45px;

          float: left;
          outline: none;
          border: 1px solid #ddd;

          box-sizing: border-box;
          -webkit-box-sizing: border-box;
          -moz-box-sizing: border-box;

          -webkit-border-radius: 5px;
          -moz-border-radius: 5px;
          border-radius: 5px;

          font-family: 'PT Sans', Helvetica, Arial, sans-serif;
          font-size: 12pt;
        }
        .search-input-container .icon {
          position: absolute;
          left: 0;
          top: 50%;
          margin-left: 17px;
          margin-top: 13px;
          z-index: 1;
          color: #93a4ad;
        }

        .docutils.field-list, .docutils.footnote {
            background-color: inherit;
        }
        .large-font {
            font-size: 1.4em !important;
        }
        .right-tag {
            float: right;
            margin-left: 36px !important;
            margin-right: 0px !important;
        }
        .short-description {
            /* We hidde description inside of the article */
            display: none;
        }
        .short-description img {
            max-height: 160px;
            max-width: 40% !important;
            margin-left: 8px;
        }
        #results-list ul.search {
            padding: 0;
            max-width: 900px;
            margin: auto;
        }

        #results-list .short-description {
            /* We show description text in the archive */
            display: block !important;
        }
        #results-list li p {
            margin-bottom: 5px;
            font-size: 0.9em;
        }
        #results-list li {
            background-color: #fff !important;
            margin-bottom: 10px;
            display: block !important;
            padding: 15px;
            border-bottom: 2px solid #ddd;
        }
        #results-list span.tag {
            display: inline-block;
            padding: 3px 4px;
            margin-right: 8px;
            position: relative;
            top: -2px;

            background: #888;
            color: #fff;

            font-size: 0.75em;
            font-weight: 600;
            line-height: 1;
            text-transform: uppercase;

            -webkit-border-radius: 2px;
            -moz-border-radius: 2px;
            border-radius: 2px;
        }
    </style></head>
    <body role="document">
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a> to improve your experience.</p>
        <![endif]-->

      <div id="container"><header role="banner">
        <div>
          <h1><a href="../../../pages/home.html">NeuPy</a></h1>
          <h2>Neural Networks in Python</h2>
        </div>
    </header>
    <nav role="navigation">
      <ul>
        <li class="main-nav">
          <a href="../../../archive.html">Articles</a>
        </li>
        <li class="main-nav">
          <a href="../../../docs/tutorials.html">Tutorials</a>
        </li>
        <li class="main-nav">
          <a href="../../../pages/documentation.html">Documentation</a>
        </li>
        <li class="main-nav">
          <a href="../../../pages/cheatsheet.html">Cheat sheet</a>
        </li>
        <li class="main-nav">
          <a href="../../../pages/model_zoo.html">Model Zoo</a>
        </li>
      </ul>
    </nav>

<div class="main-container" role="main"><div class="main wrapper body clearfix"><article><div class="timestamp postmeta">
            <span>September 20, 2015</span>
        </div>
    <div class="section" id="discrete-hopfield-network">
<span id="id1"></span><h1><a class="toc-backref" href="#id6">Discrete Hopfield Network</a></h1>
<div class="short-description">
    <img src="_static/docimg/hopfiled-weights.png" align="right">
    <p>
    In this article, we describe core ideas behind discrete hopfield networks and try to understand how it works. In addition, we explore main problems related to this algorithm. And finally, we take a look into simple example that aims to memorize digit patterns and reconstruct them from corrupted samples.
    </p>
    <br clear="right">
</div><div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#discrete-hopfield-network" id="id6">Discrete Hopfield Network</a><ul>
<li><a class="reference internal" href="#architecture" id="id7">Architecture</a><ul>
<li><a class="reference internal" href="#training-procedure" id="id8">Training procedure</a></li>
<li><a class="reference internal" href="#recovery-from-memory" id="id9">Recovery from memory</a><ul>
<li><a class="reference internal" href="#synchronous" id="id10">Synchronous</a></li>
<li><a class="reference internal" href="#asynchronous" id="id11">Asynchronous</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#memory-limit" id="id12">Memory limit</a></li>
<li><a class="reference internal" href="#hallucinations" id="id13">Hallucinations</a></li>
<li><a class="reference internal" href="#example" id="id14">Example</a></li>
<li><a class="reference internal" href="#more-reading" id="id15">More reading</a></li>
<li><a class="reference internal" href="#references" id="id16">References</a></li>
</ul>
</li>
</ul>
</div>
<p>In this article we are going to learn about <a class="reference internal" href="../../../apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> algorithm.</p>
<p><a class="reference internal" href="../../../apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> is a type of algorithms which is called - <a class="reference external" href="https://en.wikipedia.org/wiki/Autoassociative_memory">Autoassociative memories</a>
Don’t be scared of the word <cite>Autoassociative</cite>.
The idea behind this type of algorithms is very simple.
It can store useful information in <cite>memory</cite> and later it is able to reproduce this information from partially broken patterns.
You can perceive it as human memory.
For instance, imagine that you look at an old picture of a place where you were long time ago, but this picture is of very bad quality and very blurry.
By looking at the picture you manage to recognize a few objects or places that make sense to you and form some objects even though they are blurry.
It can be a house, a lake or anything that can add up to the whole picture and bring out some associations about this place.
With these details that you got from your memory so far other parts of picture start to make even more sense.
Though you don’t clearly see all objects in the picture, you start to remember things and withdraw from your memory some images, that cannot be seen in the picture, just because of those very familiarly-shaped details that you’ve got so far.
That’s what it is all about.
Autoassociative memory networks is a possibly to interpret functions of memory into neural network model.</p>
<p>Don’t worry if you have only basic knowledge in Linear Algebra; in this article I’ll try to explain the idea as simple as possible.
If you are interested in proofs of the <a class="reference internal" href="../../../apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> you can check them at R. Rojas. Neural Networks <a class="footnote-reference" href="#id3" id="id2">[1]</a> book.</p>
<div class="section" id="architecture">
<h2><a class="toc-backref" href="#id7">Architecture</a></h2>
<p><a class="reference internal" href="../../../apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> is an easy algorithm.
It’s simple because you don’t need a lot of background knowledge in Maths for using it.
Everything you need to know is how to make a basic Linear Algebra operations, like outer product or sum of two matrices.</p>
<p>Let’s begin with a basic thing.
What do we know about this neural network so far?
Just the name and the type.
From the name we can identify one useful thing about the network.
It’s <cite>Discrete</cite>.
It means that network only works with binary vectors.
But for this network we wouldn’t use binary numbers in a typical form.
Instead, we will use bipolar numbers.
They are almost the same, but instead of 0 we are going to use -1 to decode a negative state.
We can’t use zeros.
And there are two main reasons for it.
The first one is that zeros reduce information from the network weight, later in this article you are going to see it.
The second one is more complex, it depends on the nature of bipolar vectors.
Basically they are more likely to be orthogonal to each other which is a critical moment for the <a class="reference internal" href="../../../apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.
But as I mentioned before we won’t talk about proofs or anything not related to basic understanding of Linear Algebra operations.</p>
<p>So, let’s look at how we can train and use the <a class="reference internal" href="../../../apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.</p>
<div class="section" id="training-procedure">
<h3><a class="toc-backref" href="#id8">Training procedure</a></h3>
<p>We can’t use memory without any patterns stored in it.
So first of all we are going to learn how to train the network.
For the <a class="reference internal" href="../../../apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> train procedure doesn’t require any iterations.
It includes just an outer product between input vector and transposed input vector.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    W = x \cdot x^T =
    \left[
    \begin{array}{c}
      x_1\\
      x_2\\
      \vdots\\
      x_n
    \end{array}
    \right]
    \cdot
    \left[
    \begin{array}{c}
      x_1 &amp; x_2 &amp; \cdots &amp; x_n
    \end{array}
    \right]
\end{align*}
=\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    =
    \left[
    \begin{array}{c}
      x_1^2 &amp; x_1 x_2 &amp; \cdots &amp; x_1 x_n \\
      x_2 x_1 &amp; x_2^2 &amp; \cdots &amp; x_2 x_n \\
      \vdots\\
      x_n x_1 &amp; x_n x_2 &amp; \cdots &amp; x_n^2 \\
    \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(W\)</span> is a weight matrix and <span class="math notranslate nohighlight">\(x\)</span> is an input vector.
Each value <span class="math notranslate nohighlight">\(x_i\)</span> in the input vector can only be -1 or 1.
So on the matrix diagonal we only have squared values and it means we will always see 1s at those places.
Think about it, every time, in any case, values on the diagonal can take just one possible state.
We can’t use this information, because it doesn’t say anything useful about patterns that are stored in the memory and even can make incorrect contribution into the output result.
For this reason we need to set up all the diagonal values equal to zero.
The final weight formula should look like this one below.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    W =
    x x^T - I =
    \left[
    \begin{array}{c}
      0 &amp; x_1 x_2 &amp; \cdots &amp; x_1 x_n \\
      x_2 x_1 &amp; 0 &amp; \cdots &amp; x_2 x_n \\
      \vdots\\
      x_n x_1 &amp; x_n x_2 &amp; \cdots &amp; 0 \\
    \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>Where <span class="math notranslate nohighlight">\(I\)</span> is an identity matrix.</p>
<p>But usually we need to store more values in memory.
For another pattern we have to do exactly the same procedure as before and then just add the generated weight matrix to the old one.</p>
<div class="math notranslate nohighlight">
\[W = W_{old} + W_{new}\]</div>
<p>And this procedure generates us a new weight that would be valid for both previously stored patterns.
Later you can add other patterns using the same algorithm.</p>
<p>But if you need to store multiple vectors inside the network at the same time you don’t need to compute the weight for each vector and then sum them up.
If you have a matrix <span class="math notranslate nohighlight">\(X \in \Bbb R^{m\times n}\)</span> where each row is the input vector, then you can just make product matrix between transposed input matrix and input matrix.</p>
<div class="math notranslate nohighlight">
\[W = X^T X - m I\]</div>
<p>Where <span class="math notranslate nohighlight">\(I\)</span> is an identity matrix (<span class="math notranslate nohighlight">\(I \in \Bbb R^{n\times n}\)</span>), <span class="math notranslate nohighlight">\(n\)</span> is a number of features in the input vector and <span class="math notranslate nohighlight">\(m\)</span> is a number of input patterns inside the matrix <span class="math notranslate nohighlight">\(X\)</span>.
Term <span class="math notranslate nohighlight">\(m I\)</span> removes all values from the diagonal.
Basically we remove 1s for each stored pattern and since we have <span class="math notranslate nohighlight">\(m\)</span> of them, we should do it <span class="math notranslate nohighlight">\(m\)</span> times.
Practically, it’s not very good to create an identity matrix just to set up zeros on the diagonal, especially when dimension on the matrix is very big.
Usually linear algebra libraries give you a possibility to set up diagonal values without creating an additional matrix and this solution would be more efficient.
For example in NumPy library it’s a <a class="reference external" href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.fill_diagonal.html">numpy.fill_diagonal</a> function.</p>
<p>Let’s check an example just to make sure that everything is clear.
Let’s pretend we have a vector <span class="math notranslate nohighlight">\(u\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}u = \left[\begin{align*}1 \\ -1 \\ 1 \\ -1\end{align*}\right]\end{split}\]</div>
<p>Assume that network doesn’t have patterns inside of it, so the vector <span class="math notranslate nohighlight">\(u\)</span> would be the first one.
Let’s compute weights for the network.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    U = u u^T =
    \left[
        \begin{array}{c}
            1 \\
            -1 \\
            1 \\
            -1
        \end{array}
    \right]
    \left[
        \begin{array}{c}
            1 &amp; -1 &amp; 1 &amp; -1
        \end{array}
    \right]
    =
    \left[
        \begin{array}{cccc}
            1 &amp; -1 &amp; 1 &amp; -1\\
            -1 &amp; 1 &amp; -1 &amp; 1\\
            1 &amp; -1 &amp; 1 &amp; -1\\
            -1 &amp; 1 &amp; -1 &amp; 1
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>Look closer to the matrix <span class="math notranslate nohighlight">\(U\)</span> that we got.
Outer product just repeats vector 4 times with the same or inversed values.
First and third columns (or rows, it doesn’t matter, because matrix is symmetrical) are exactly the same as the input vector.
The second and fourth are also the same, but with an opposite sign.
That’s because in the vector <span class="math notranslate nohighlight">\(u\)</span> we have 1 on the first and third places and -1 on the other.</p>
<p>To make weight from the <span class="math notranslate nohighlight">\(U\)</span> matrix, we need to remove ones from the diagonal.</p>
<div class="math notranslate nohighlight">
\[\begin{split}W = U - I = \left[
    \begin{array}{cccc}
        1 &amp; -1 &amp; 1 &amp; -1\\
        -1 &amp; 1 &amp; -1 &amp; 1\\
        1 &amp; -1 &amp; 1 &amp; -1\\
        -1 &amp; 1 &amp; -1 &amp; 1
    \end{array}
\right] -
\left[
    \begin{array}{cccc}
        1 &amp; 0 &amp; 0 &amp; 0\\
        0 &amp; 1 &amp; 0 &amp; 0\\
        0 &amp; 0 &amp; 1 &amp; 0\\
        0 &amp; 0 &amp; 0 &amp; 1
    \end{array}
\right] =\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}= \left[
    \begin{array}{cccc}
        0 &amp; -1 &amp; 1 &amp; -1\\
        -1 &amp; 0 &amp; -1 &amp; 1\\
        1 &amp; -1 &amp; 0 &amp; -1\\
        -1 &amp; 1 &amp; -1 &amp; 0
    \end{array}
\right]\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(I\)</span> is the identity matrix and <span class="math notranslate nohighlight">\(I \in \Bbb R^{n \times n}\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is a number of features in the input vector.</p>
<p>When we have one stored vector inside the weights we don’t really need to remove 1s from the diagonal.
The main problem would appear when we have more than one vector stored in the weights.
Each value on the diagonal would be equal to the number of stored vectors in it.</p>
</div>
<div class="section" id="recovery-from-memory">
<h3><a class="toc-backref" href="#id9">Recovery from memory</a></h3>
<p>The main advantage of Autoassociative network is that it is able to recover pattern from the memory using just a partial information about the pattern.
There are already two main approaches to this situation, synchronous and asynchronous. We are going to master both of them.</p>
<div class="section" id="synchronous">
<h4><a class="toc-backref" href="#id10">Synchronous</a></h4>
<p>Synchronous approach is much more easier for understanding, so we are going to look at it firstly. To recover your pattern from memory you just need to multiply the weight matrix by the input vector.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    s = {W}\cdot{x}=
    \left[
    \begin{array}{cccc}
      w_{11} &amp; w_{12} &amp; \ldots &amp; w_{1n}\\
      w_{21} &amp; w_{22} &amp; \ldots &amp; w_{2n}\\
      \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
      w_{n1} &amp; w_{n2} &amp; \ldots &amp; w_{nn}
    \end{array}
    \right]
    \left[
    \begin{array}{c}
      x_1\\
      x_2\\
      \vdots\\
      x_n
    \end{array}
    \right]
    =
\end{align*}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    =
    \left[
        \begin{array}{c}
          w_{11}x_1+w_{12}x_2 + \cdots + w_{1n} x_n\\
          w_{21}x_1+w_{22}x_2 + \cdots + w_{2n} x_n\\
          \vdots\\
          w_{n1}x_1+w_{n2}x_2 + \cdots + w_{nn} x_n\\
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>Let’s analyze the result.
We summed up all information from the weights where each value can be any integer with an absolute value equal to or smaller than the number of patterns inside the network.
It’s clear that total sum value for <span class="math notranslate nohighlight">\(s_i\)</span> is not necessary equal to -1 or 1, so we have to make additional operations that will make bipolar vector from the vector <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p>Let’s think about this product operation.
What does it actualy do?
Basically after training procedure we saved our pattern dublicated <span class="math notranslate nohighlight">\(n\)</span> times (where <span class="math notranslate nohighlight">\(n\)</span> is a number of input vector features) inside the weight.
When we store more patterns we get interception between them (it’s called a <strong>crosstalk</strong>) and each pattern add some noise to other patterns.
So, after perfoming product matrix between <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(x\)</span> for each value from the vector <span class="math notranslate nohighlight">\(x\)</span> we’ll get a recovered vector with a little bit of noise.
For <span class="math notranslate nohighlight">\(x_1\)</span> we get a first column from the matrix <span class="math notranslate nohighlight">\(W\)</span>, for the <span class="math notranslate nohighlight">\(x_2\)</span> a second column, and so on.
Then we sum up all vectors together.
This operation can remind you of voting.
For example we have 3 vectors.
If the first two vectors have 1 in the first position and the third one has -1 at the same position, the winner should be 1.
We can perform the same procedure with <span class="math notranslate nohighlight">\(sign\)</span> function.
So the output value should be 1 if total value is greater then zero and -1 otherwise.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}sign(x) = \left\{
    \begin{array}{lr}
        &amp;1 &amp;&amp; : x \ge 0\\
        &amp;-1 &amp;&amp; : x &lt; 0
    \end{array}
\right.\\\end{split}\\y = sign(s)\end{aligned}\end{align} \]</div>
<p>That’s it.
Now <span class="math notranslate nohighlight">\(y\)</span> store the recovered pattern from the input vector <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>Maybe now you can see why we can’t use zeros in the input vectors.
In <cite>voting</cite> procedure we use each row that was multiplied by bipolar number, but if values had been zeros they would have ignored columns from the weight matrix and we would have used only values related to ones in the input pattern.</p>
<p>Of course you can use 0 and 1 values and sometime you will get the correct result, but this approach give you much worse results than explained above.</p>
</div>
<div class="section" id="asynchronous">
<h4><a class="toc-backref" href="#id11">Asynchronous</a></h4>
<p>Previous approach is good, but it has some limitations. If you change one value in the input vector it can change your output result and value won’t converge to the known pattern. Another popular approach is an <strong>asynchronous</strong>. This approach is more likely to remind you of real memory. At the same time in network activates just one random neuron instead of all of them. In terms of neural networks we say that <strong>neuron fires</strong>. We iteratively repeat this operation multiple times and after some point network will converge to some pattern.</p>
<p>Let’s look at this example: Consider that we already have a weight matrix <span class="math notranslate nohighlight">\(W\)</span> with one pattern <span class="math notranslate nohighlight">\(x\)</span>  inside of it.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    W =
    \left[
    \begin{array}{cccc}
      0 &amp; 1 &amp; -1 \\
      1 &amp; 0 &amp; -1 \\
      -1 &amp; -1 &amp; 0
    \end{array}
    \right]
\end{align*}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    x =
    \left[
        \begin{array}{c}
          1\\
          1\\
          -1
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>Let’s assume that we have a vector <span class="math notranslate nohighlight">\(x^{'}\)</span> from which we want to recover the pattern.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    x^{'} =
    \left[
        \begin{array}{c}
          1\\
          -1\\
          -1
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>In first iteration one neuron fires. Let it be the second one. So we multiply the first column by this selected value.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    x^{'}_2 =
    sign(\left[
        \begin{array}{c}
          1 &amp; -1 &amp; -1
        \end{array}
    \right] \cdot \left[
        \begin{array}{c}
          1\\
          0\\
          -1
        \end{array}
    \right]) = sign(2) = 1
\end{align*}\end{split}\]</div>
<p>And after this operation we set up a new value into the input vector <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    x^{'} =
    \left[
        \begin{array}{c}
          1\\
          1\\
          -1
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>As you can see, after first iteration value is exactly the same as <span class="math notranslate nohighlight">\(x\)</span> but we can keep going. In second iteration random neuron fires again. Let’s pretend that this time it was the third neuron.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    x^{'}_3 =
    sign(\left[
        \begin{array}{c}
          1 &amp; 1 &amp; -1
        \end{array}
    \right] \cdot \left[
        \begin{array}{c}
          -1\\
          -1\\
          0
        \end{array}
    \right]) = sign(-2) = -1
\end{align*}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(x^{'}_3\)</span> is exactly the same as in the <span class="math notranslate nohighlight">\(x^{'}\)</span> vector so we don’t need to update it. We can repeat it as many times as we want, but we will be getting the same value.</p>
</div>
</div>
</div>
<div class="section" id="memory-limit">
<h2><a class="toc-backref" href="#id12">Memory limit</a></h2>
<p>Obviously, you can’t store infinite number of vectors inside the network.
There are two good rules of thumb.</p>
<p>Consider that <span class="math notranslate nohighlight">\(n\)</span> is the dimension (number of features) of your input vector and <span class="math notranslate nohighlight">\(m\)</span> is the number of patterns that you want to store in the network.
The first rule gives us a simple ration between <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(n\)</span>.</p>
<div class="math notranslate nohighlight">
\[m \approx 0.18 n\]</div>
<p>The main problem with this rule is that proof assumes that stored vectors inside the weight are completely random with an equal probability.
Unfortunately, that is not always true.
Let’s suppose we save some images of numbers from 0 to 9.
Pictures are black and white, so we can encode them in bipolar vectors.
Will the probabilities be the same for seeing as many white pixels as black ones?
Usually no.
More likely that number of white pixels would be greater than number of black ones.
Before use this rule you have to think about type of your input patterns.</p>
<p>The second rule uses a logarithmic proportion.</p>
<div class="math notranslate nohighlight">
\[m = \left \lfloor \frac{n}{2 \cdot log(n)} \right \rfloor\]</div>
<p>Both of these rules are good assumptions about the nature of data and its possible limits in memory. Of course, you can find situations when these rules will fail.</p>
</div>
<div class="section" id="hallucinations">
<h2><a class="toc-backref" href="#id13">Hallucinations</a></h2>
<p>Hallucinations is one of the main problems in the <a class="reference internal" href="../../../apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.
Sometimes network output can be something that we hasn’t taught it.</p>
<p>To understand this phenomena we should firstly define the Hopfield energy function.</p>
<div class="math notranslate nohighlight">
\[E = -\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij} x_i x_j + \sum_{i=1}^{n} \theta_i x_i\]</div>
<p>Where <span class="math notranslate nohighlight">\(w_{ij}\)</span> is a weight value on the <span class="math notranslate nohighlight">\(i\)</span>-th row and <span class="math notranslate nohighlight">\(j\)</span>-th column.
<span class="math notranslate nohighlight">\(x_i\)</span> is a <span class="math notranslate nohighlight">\(i\)</span>-th values from the input vector <span class="math notranslate nohighlight">\(x\)</span>.
<span class="math notranslate nohighlight">\(\theta\)</span> is a threshold.
Threshold defines the bound to the sign function.
For this reason <span class="math notranslate nohighlight">\(\theta\)</span> is equal to 0 for the <a class="reference internal" href="../../../apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.
In terms of a linear algebra we can write formula for the <a class="reference internal" href="../../../apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> energy Function more simpler.</p>
<div class="math notranslate nohighlight">
\[E = -\frac{1}{2} x^T W x\]</div>
<p>But linear algebra notation works only with the <span class="math notranslate nohighlight">\(x\)</span> vector, we can’t use matrix <span class="math notranslate nohighlight">\(X\)</span> with multiple input patterns instead of the <span class="math notranslate nohighlight">\(x\)</span> in this formula.
For the energy function we’re always interested in finding a minimum value, for this reason it has minus sign at the beginning.</p>
<p>Let’s try to visualize it.
Assume that values for vector <span class="math notranslate nohighlight">\(x\)</span> can be continous in order and we can visualize them using two parameters.
Let’s pretend that we have two vectors <cite>[1, -1]</cite> and <cite>[-1, 1]</cite> stored inside the network.
Below you can see the plot that visualizes energy function for this situation.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/energy-function.png"><img alt="Energy function visualization for the network with two neurons" src="../../../_images/energy-function.png" style="width: 80%;" /></a>
</div>
<p>As you can see we have two minimum values at the same points as those patterns that are already stored inside the network.
But between these two patterns function creates a saddle point somewhere at the point with coordinates <span class="math notranslate nohighlight">\((0, 0)\)</span>.
In this case we can’t stick to the points <span class="math notranslate nohighlight">\((0, 0)\)</span>.
But in situation with more dimensions this saddle points can be at the level of available values and they could be hallucination.
Unfortunately, we are very limited in terms of numbers of dimensions we could plot, but the problem is still the same.</p>
<p>Full source code for this plot you can find on <a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/memory/dhn_energy_func.py">github</a></p>
</div>
<div class="section" id="example">
<h2><a class="toc-backref" href="#id14">Example</a></h2>
<p>Now we are ready for a more practical example.
Let’s define a few images that we are going to teach the network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">draw_bin_image</span><span class="p">(</span><span class="n">image_matrix</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">image_matrix</span><span class="o">.</span><span class="n">tolist</span><span class="p">():</span>
<span class="gp">... </span>        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;| &#39;</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39; *&#39;</span><span class="p">[</span><span class="n">val</span><span class="p">]</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">row</span><span class="p">))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">zero</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span>
<span class="gp">... </span><span class="p">])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">one</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
<span class="gp">... </span><span class="p">])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">two</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span><span class="p">])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">zero</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * * *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">|   * * *</span>
</pre></div>
</div>
<p>We have 3 images, so now we can train network with these patterns.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">zero</span><span class="p">,</span> <span class="n">one</span><span class="p">,</span> <span class="n">two</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">DiscreteHopfieldNetwork</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;sync&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>That’s all. Now to make sure that network has memorized patterns right we can define the broken patterns and check how the network will recover them.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">half_zero</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">half_zero</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * * *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">|</span>
<span class="go">|</span>
<span class="go">|</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">half_two</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">half_two</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|</span>
<span class="go">|</span>
<span class="go">|</span>
<span class="go">|   * *</span>
<span class="go">| *</span>
<span class="go">| * * * * *</span>
</pre></div>
</div>
<p>Now we can reconstruct pattern from the memory.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_zero</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * * *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">|   * * *</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_two</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">| * * *</span>
<span class="go">|       *</span>
<span class="go">|       *</span>
<span class="go">|   * *</span>
<span class="go">| *</span>
<span class="go">| * * * * *</span>
</pre></div>
</div>
<p>Cool! Network catches the pattern right.</p>
<p>But not always we will get the correct answer. Let’s define another broken pattern and check network output.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">half_two</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span><span class="p">])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_two</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="go">|   * *</span>
<span class="go">| *   *</span>
<span class="go">| * * * * *</span>
</pre></div>
</div>
<p>We hasn’t clearly taught the network to deal with such pattern. But if we look closer, it looks like mixed pattern of numbers 1 and 2.</p>
<p>This problem we can solve using the asynchronous network approach. We don’t necessary need to create a new network, we can just simply switch its mode.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">utils</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">utils</span><span class="o">.</span><span class="n">reproducible</span><span class="p">()</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="s1">&#39;async&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span><span class="o">.</span><span class="n">n_times</span> <span class="o">=</span> <span class="mi">400</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_two</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_two</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">| * * *</span>
<span class="go">|       *</span>
<span class="go">|       *</span>
<span class="go">|   * *</span>
<span class="go">| *</span>
<span class="go">| * * * *</span>
</pre></div>
</div>
<p>Our broken pattern is really close to the minimum of 1 and 2 patterns. Randomization helps us choose direction but it’s not necessary the right one, especially when the broken pattern is close to 1 and 2 at the same time.</p>
<p>Check last output with number two again. Is that a really valid pattern for number 2? Final symbol in output is wrong. We are not able to recover patter 2 from this network, because input vector is always much closer to the minimum that looks very similar to pattern 2.</p>
<p>In plot below you can see first 200 iterations of the recovery procedure. Energy value was decreasing after each iteration until it reached the local minimum where pattern is equal to 2.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/hopfield-energy-vis.png"><img alt="Asynchronous Discrete Hopfield Network energy update after each iteration" src="../../../_images/hopfield-energy-vis.png" style="width: 80%;" /></a>
</div>
<p>And finally we can look closer to the network memory using Hinton diagram.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">plots</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Hinton diagram&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plots</span><span class="o">.</span><span class="n">hinton</span><span class="p">(</span><span class="n">dhnet</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/hinton-diagram.png"><img alt="Asynchronous Discrete Hopfield Network energy update after each iteration" src="../../../_images/hinton-diagram.png" style="width: 80%;" /></a>
</div>
<p>This graph above shows the network weight matrix and all information stored inside of it. Hinton diagram is a very simple technique for the weight visualization in neural networks. Each value encoded in square where its size is an absolute value from the weight matrix and color shows the sign of this value. White is a positive and black is a negative. Usually Hinton diagram helps identify some patterns in the weight matrix.</p>
<p>Let’s go back to the graph. What can you say about the network just by looking at this picture? First of all you can see that there is no squares on the diagonal. That is because they are equal to zero. The second important thing you can notice is that the plot is symmetrical. But that is not all that you can withdraw from the graph. Can you see different patterns? You can find rows or columns with exactly the same values, like the second and third columns. Fifth column is also the same but its sign is reversed. Now look closer to the antidiagonal. What can you say about it? If you are thinking that all squares are white - you are right. But why is that true? Is there always the same patterns for each memory matrix? No, it is a special property of patterns that we stored inside of it. If you draw a horizontal line in the middle of each image and look at it you will see that values are opposite symmetric. For instance, <span class="math notranslate nohighlight">\(x_1\)</span> opposite symmetric to <span class="math notranslate nohighlight">\(x_{30}\)</span>, <span class="math notranslate nohighlight">\(x_2\)</span> to <span class="math notranslate nohighlight">\(x_{29}\)</span>, <span class="math notranslate nohighlight">\(x_3\)</span> to <span class="math notranslate nohighlight">\(x_{28}\)</span> and so on. Zero pattern is a perfect example where each value have exactly the same opposite symmetric pair. One is almost perfect except one value on the <span class="math notranslate nohighlight">\(x_2\)</span> position. Two is not clearly opposite symmetric. But if you check each value you will find that more than half of values are symmetrical. Combination of those patterns gives us a diagonal with all positive values. If we have all perfectly opposite symmetric patterns then squares on the antidiagonal will have the same length, but in this case pattern for number 2 gives a little bit of noise and squares have different sizes.</p>
<p>Properties that we’ve reviewed so far are just the most interesting and maybe other patterns you can encounter on your own.</p>
</div>
<div class="section" id="more-reading">
<h2><a class="toc-backref" href="#id15">More reading</a></h2>
<p>In addition you can read another article about a ‘<a class="reference internal" href="../21/password_recovery.html#password-recovery"><span class="std std-ref">Password recovery</span></a>’ from the memory using the <a class="reference internal" href="../../../apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.</p>
</div>
<div class="section" id="references">
<h2><a class="toc-backref" href="#id16">References</a></h2>
<table class="docutils footnote" frame="void" id="id3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[1]</a></td><td>R. Rojas. Neural Networks. In Associative Networks. pp. 311 - 336, 1996.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Math4IQB. (2013, November 17). Hopfield Networks. Retrieved
from <a class="reference external" href="https://www.youtube.com/watch?v=gfPUWwBkXZY">https://www.youtube.com/watch?v=gfPUWwBkXZY</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>R. Callan. The Essence of Neural Networks. In Pattern Association. pp. 84 - 98, 1999.</td></tr>
</tbody>
</table>
</div>
</div>

    <div class="postmeta">
        <div class="author">
            <span>Posted by Yurii Shevchuk</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="../../../tags/memory.html">memory</a>, <a href="../../../tags/unsupervised.html">unsupervised</a>, <a href="../../../tags/discrete_hopfield_network.html">discrete hopfield network</a></span>
        </div>
        </div><ul class="related clearfix">
            <li class="left"> &laquo; <a href="../21/password_recovery.html">Password recovery</a></li>
            <li class="right"><a href="../../07/04/boston_house_prices_dataset.html">Predict prices for houses in the area of Boston</a> &raquo; </li>
        </ul><div id="disqus_thread"></div><script type="text/javascript">    var disqus_shortname = "neupy";    var disqus_identifier = "2015/09/20/discrete_hopfield_network";    disqus_thread();</script><noscript>Please enable JavaScript to view the    <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></article><aside class="sidebar"><section><div class="widget" id="searchbox" role="search">
    <h1><a href="#searchbox">Search</a></h1>
    <form action="../../../search.html" method="get">
          <div class="box">
            <div class="search-input-container">
                <span class="icon"><i class="fa fa-search"></i></span>
                <input type="search" name="q" id="search" placeholder="Search..." />
            </div>
          </div>
    </form>
</div></section><section><div class="widget">
    <h1>Install NeuPy</h1>
    <div class="highligh-bash">
        <div class="highlight">
            <pre>pip install neupy</pre>
        </div>
    </div>
    <p>
        <div>Learn more about NeuPy reading <a href="../../../docs/tutorials.html">tutorials</a> and <a href="../../../pages/documentation.html">documentation</a>.</div>
    </p>
</div></section><section><div class="widget">
    <h1>Issues and feature requests</h1>
    <p>
        If you find a bug or want to suggest a new feature feel free to
        <a href="https://github.com/itdxer/neupy/issues/new">create an issue</a>
        on Github
    </p>
</div></section></aside></div> <!-- #main --></div> <!-- #main-container -->

        <div class="footer-container" role="contentinfo"><footer class="wrapper">&copy; Copyright 2015 - 2019, Yurii Shevchuk. Powered by <a href="http://www.tinkerer.me/">Tinkerer</a> and <a href="http://sphinx.pocoo.org/">Sphinx</a>.</footer></div> <!-- footer-container -->

      </div> <!--! end of #container --><!--[if lt IE 7 ]>
          <script src="//ajax.googleapis.com/ajax/libs/chrome-frame/1.0.3/CFInstall.min.js"></script>
          <script>window.attachEvent('onload',function(){CFInstall.check({mode:'overlay'})})</script>
        <![endif]-->
    </body>
</html>