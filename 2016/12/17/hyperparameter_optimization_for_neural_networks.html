<!DOCTYPE html><!--[if lt IE 7]>      <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="description" content="NeuPy is a Python library for Artificial Neural Networks. NeuPy supports many different types of Neural Networks from a simple perceptron to deep learning models.">
        <meta name="viewport" content="width=device-width">
        <title>Hyperparameter optimization for Neural Networks &mdash; NeuPy</title>
            <link rel="stylesheet" href="../../../_static/normalize.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/sphinx.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/main.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/flat.css" type="text/css">
            <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
            <link rel="stylesheet" href="../../../_static/font-awesome.min.css" type="text/css">
        <link rel="shortcut icon" href="../../../_static/favicon.ico" /><!-- Load modernizr and JQuery -->
        <script src="../../../_static/vendor/modernizr-2.6.2.min.js"></script>
        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"></script>
        <script>window.jQuery || document.write('<script src="../../../_static/vendor/jquery-1.8.2.min.js"><\/script>')</script>
        <script src="../../../_static/plugins.js"></script>
        <script src="../../../_static/main.js"></script>
        <link rel="search" title="Search" href="../../../search.html" /><link rel="next" title="Image classification, MNIST digits" href="../../11/12/mnist_classification.html" /><link rel="prev" title="Self-Organizing Maps and Applications" href="../../../2017/12/09/sofm_applications.html" /><link rel="alternate" type="application/rss+xml" title="RSS" href="../../../rss.html" /><script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '1.7.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        SOURCELINK_SUFFIX: '.txt',
        HAS_SOURCE:  true
      };
    </script><script type="text/javascript" src="../../../_static/underscore.js"></script><script type="text/javascript" src="../../../_static/doctools.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="../../../_static/disqus.js"></script><script type="text/javascript" src="../../../_static/js/google_analytics.js"></script><script type="text/javascript" src="../../../_static/js/script.js"></script><script type="text/javascript" src="../../../_static/js/copybutton.js"></script>

    <script type="text/javascript">
        $(document).ready(function () {
            // Scroll to content if on small screen
            if (screen.width < 480)
            {
                $(document).scrollTop(document.getElementsByTagName("article")[0].offsetTop - 44);
            }
        });
    </script>
    <style media="screen" type="text/css">
        img { max-width: 800px !important; }
        img[src="../_images/mnist-code-sample-home.png"] { max-width: 600px !important; }
        table { background-color: white; }
        table.docutils.citation  { background-color: inherit; }
        div.highlight { margin-bottom: 20px !important; }

        .limited-width { max-width: 800px; margin: auto; }
        .docutils { width: 100%; }
        a .docutils.literal {
            background-color: inherit !important;
            padding: 0px !important;
            color: #3bbc46 !important;
        }
        .docutils td { padding: 10px; }
        .section { word-wrap:break-word; }
        .descname { font-weight: bold; }
        .highlight-python + .figure { margin-top: 20px; }
        .dataframe { text-align: center !important; width: 100%; margin: 10px 0 10px 0; }
        .dataframe td { padding: 5px; }

        .math .gd { color: #000 !important; } /* Generic.Deleted */
        .math .m { color: #000 !important; } /* Literal.Number */
        .math .s { color: #000 !important; } /* Literal.String */
        .math .mf { color: #000 !important; } /* Literal.Number.Float */
        .math .mh { color: #000 !important; } /* Literal.Number.Hex */
        .math .mi { color: #000 !important; } /* Literal.Number.Integer */
        .math .mo { color: #000 !important; } /* Literal.Number.Oct */
        .math .sc { color: #000 !important; } /* Literal.String.Char */
        .math .s2 { color: #000 !important; } /* Literal.String.Double */
        .math .si { color: #000 !important; } /* Literal.String.Interpol */
        .math .sx { color: #000 !important; } /* Literal.String.Other */
        .math .s1 { color: #000 !important; } /* Literal.String.Single */
        .math .ss { color: #000 !important; } /* Literal.String.Symbol */
        .math .il { color: #000 !important; } /* Literal.Number.Integer.Long */

        /* Background for class and function names */
        dt[id^="neupy."] {
            background-color: #e6edf2;
            border: 1px solid #f8fafb;
            border-radius: 8px;
            padding: 10px 20px;
        }
        div[id^="module-neupy."] h1 {
            display: none;
        }

        /* Search input field */
        .search-input {
            width: 100%;
            padding: 10px;
            display: block;
        }
        .box {
          padding-bottom: 50px;
        }
        .search-input-container {
          width: 100%;
          vertical-align: middle;
          white-space: nowrap;
          position: relative;
        }
        .search-input-container input#search {
          width: 100%;
          height: 50px;
          padding-left: 45px;

          float: left;
          outline: none;
          border: 1px solid #ddd;

          box-sizing: border-box;
          -webkit-box-sizing: border-box;
          -moz-box-sizing: border-box;

          -webkit-border-radius: 5px;
          -moz-border-radius: 5px;
          border-radius: 5px;

          font-family: 'PT Sans', Helvetica, Arial, sans-serif;
          font-size: 12pt;
        }
        .search-input-container .icon {
          position: absolute;
          left: 0;
          top: 50%;
          margin-left: 17px;
          margin-top: 13px;
          z-index: 1;
          color: #93a4ad;
        }

        .docutils.field-list, .docutils.footnote {
            background-color: inherit;
        }
        .large-font {
            font-size: 1.4em !important;
        }
        .right-tag {
            float: right;
            margin-left: 36px !important;
            margin-right: 0px !important;
        }
        .short-description {
            /* We hidde description inside of the article */
            display: none;
        }
        .short-description img {
            max-height: 160px;
            max-width: 40% !important;
            margin-left: 8px;
        }
        #results-list ul.search {
            padding: 0;
            max-width: 900px;
            margin: auto;
        }

        #results-list .short-description {
            /* We show description text in the archive */
            display: block !important;
        }
        #results-list li p {
            margin-bottom: 5px;
            font-size: 0.9em;
        }
        #results-list li {
            background-color: #fff !important;
            margin-bottom: 10px;
            display: block !important;
            padding: 15px;
            border-bottom: 2px solid #ddd;
        }
        #results-list span.tag {
            display: inline-block;
            padding: 3px 4px;
            margin-right: 8px;
            position: relative;
            top: -2px;

            background: #888;
            color: #fff;

            font-size: 0.75em;
            font-weight: 600;
            line-height: 1;
            text-transform: uppercase;

            -webkit-border-radius: 2px;
            -moz-border-radius: 2px;
            border-radius: 2px;
        }
    </style></head>
    <body role="document">
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a> to improve your experience.</p>
        <![endif]-->

      <div id="container"><header role="banner">
        <div>
          <h1><a href="../../../pages/home.html">NeuPy</a></h1>
          <h2>Neural Networks in Python</h2>
        </div>
    </header>
    <nav role="navigation">
      <ul>
        <li class="main-nav">
          <a href="../../../archive.html">Articles</a>
        </li>
        <li class="main-nav">
          <a href="../../../docs/tutorials.html">Tutorials</a>
        </li>
        <li class="main-nav">
          <a href="../../../pages/documentation.html">Documentation</a>
        </li>
        <li class="main-nav">
          <a href="../../../pages/cheatsheet.html">Cheat sheet</a>
        </li>
        <li class="main-nav">
          <a href="../../../pages/model_zoo.html">Model Zoo</a>
        </li>
      </ul>
    </nav>

<div class="main-container" role="main"><div class="main wrapper body clearfix"><article><div class="timestamp postmeta">
            <span>December 17, 2016</span>
        </div>
    <div class="section" id="hyperparameter-optimization-for-neural-networks">
<h1><a class="toc-backref" href="#id13">Hyperparameter optimization for Neural Networks</a></h1>
<div class="short-description">
    <img src="https://raw.githubusercontent.com/itdxer/neupy/master/site/_static/intro/hyperopt-2-intro.png" align="right">
    <p>
    This article explains different hyperparameter algorithms that can be used for neural networks. It covers simple algorithms like Grid Search, Random Search and more complicated algorithms like Gaussian Process and Tree-structured Parzen Estimators (TPE).
    </p>
    <br clear="right">
</div><div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#hyperparameter-optimization-for-neural-networks" id="id13">Hyperparameter optimization for Neural Networks</a><ul>
<li><a class="reference internal" href="#introduction" id="id14">Introduction</a></li>
<li><a class="reference internal" href="#hyperparameter-optimization" id="id15">Hyperparameter optimization</a></li>
<li><a class="reference internal" href="#grid-search" id="id16">Grid Search</a></li>
<li><a class="reference internal" href="#random-search" id="id17">Random Search</a></li>
<li><a class="reference internal" href="#hand-tuning" id="id18">Hand-tuning</a></li>
<li><a class="reference internal" href="#bayesian-optimization" id="id19">Bayesian Optimization</a><ul>
<li><a class="reference internal" href="#gaussian-process" id="id20">Gaussian Process</a></li>
<li><a class="reference internal" href="#acquisition-function" id="id21">Acquisition Function</a></li>
<li><a class="reference internal" href="#find-number-of-hidden-units" id="id22">Find number of hidden units</a></li>
<li><a class="reference internal" href="#disadvantages-of-gp-with-ei" id="id23">Disadvantages of GP with EI</a></li>
</ul>
</li>
<li><a class="reference internal" href="#tree-structured-parzen-estimators-tpe" id="id24">Tree-structured Parzen Estimators (TPE)</a><ul>
<li><a class="reference internal" href="#overview" id="id25">Overview</a></li>
<li><a class="reference internal" href="#hyperparameter-optimization-for-mnist-dataset" id="id26">Hyperparameter optimization for MNIST dataset</a></li>
<li><a class="reference internal" href="#disadvantages-of-tpe" id="id27">Disadvantages of TPE</a></li>
</ul>
</li>
<li><a class="reference internal" href="#summary" id="id28">Summary</a></li>
<li><a class="reference internal" href="#source-code" id="id29">Source Code</a></li>
<li><a class="reference internal" href="#references" id="id30">References</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="introduction">
<h2><a class="toc-backref" href="#id14">Introduction</a></h2>
<p>Sometimes it can be difficult to choose a correct architecture for Neural Networks. Usually, this process requires a lot of experience because networks include many parameters. Let’s check some of the most important parameters that we can optimize for the neural network:</p>
<ul class="simple">
<li>Number of layers</li>
<li>Different parameters for each layer (number of hidden units, filter size for convolutional layer and so on)</li>
<li>Type of activation functions</li>
<li>Parameter initialization method</li>
<li>Learning rate</li>
<li>Loss function</li>
</ul>
<p>Even though the list of parameters in not even close to being complete, it’s still impressive how many parameters influences network’s performance.</p>
</div>
<div class="section" id="hyperparameter-optimization">
<h2><a class="toc-backref" href="#id15">Hyperparameter optimization</a></h2>
<p>In this article, I would like to show a few different hyperparameter selection methods.</p>
<ul class="simple">
<li>Grid Search</li>
<li>Random Search</li>
<li>Hand-tuning</li>
<li>Gaussian Process with Expected Improvement</li>
<li>Tree-structured Parzen Estimators (TPE)</li>
</ul>
</div>
<div class="section" id="grid-search">
<h2><a class="toc-backref" href="#id16">Grid Search</a></h2>
<p>The simplest algorithms that you can use for hyperparameter optimization is a Grid Search. The idea is simple and straightforward. You just need to define a set of parameter values, train model for all possible parameter combinations and select the best one. This method is a good choice only when model can train quickly, which is not the case for typical neural networks.</p>
<p>Imagine that we need to optimize 5 parameters. Let’s assume, for simplicity, that we want to try 10 different values per each parameter. Therefore, we need to make 100,000 (<span class="math notranslate nohighlight">\(10 ^ 5\)</span>) evaluations. Assuming that network trains 10 minutes on average we will have finished hyperparameter tuning in almost 2 years. Seems crazy, right? Typically, network trains much longer and we need to tune more hyperparameters, which means that it can take forever to run grid search for typical neural network. The better solution is random search.</p>
</div>
<div class="section" id="random-search">
<h2><a class="toc-backref" href="#id17">Random Search</a></h2>
<p>The idea is similar to Grid Search, but instead of trying all possible combinations we will just use randomly selected subset of the parameters. Instead of trying to check 100,000 samples we can check only 1,000 of parameters. Now it should take a week to run hyperparameter optimization instead of 2 years.</p>
<p>Let’s sample 100 two-dimensional data points from a uniform distribution.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/100-uniform-data-points.png"><img alt="Randomly generated 100 data points" src="../../../_images/100-uniform-data-points.png" style="width: 100%;" /></a>
</div>
<p>In case if there are not enough data points, random sampling doesn’t fully covers parameter space. It can be seen in the figure above because there are some regions that don’t have data points. In addition, it samples some points very close to each other which are redundant for our purposes. We can solve this problem with <a class="reference external" href="https://en.wikipedia.org/wiki/Low-discrepancy_sequence">Low-discrepancy sequences</a> (also called quasi-random sequences).</p>
<p>There are many different techniques for quasi-random sequences:</p>
<ul class="simple">
<li>Sobol sequence</li>
<li>Hammersley set</li>
<li>Halton sequence</li>
<li>Poisson disk sampling</li>
</ul>
<p>Let’s compare some of the mentioned methods with previously random sampled data points.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/100-random-points.png"><img alt="Randomly generated 100 data points" src="../../../_images/100-random-points.png" style="width: 100%;" /></a>
</div>
<p>As we can see now sampled points spread out through the parameter space more uniformly. One disadvantage of these methods is that not all of them can provide you good results for the higher dimensions. For instance, Halton sequence and Hammersley set do not work well for dimension bigger than 10 <a class="footnote-reference" href="#id11" id="id1">[7]</a>.</p>
<p>Even though we improved hyperparameter optimization algorithm it still is not suitable for large neural networks.</p>
<p>But before we move on to more complicated methods I want to focus on parameter hand-tuning.</p>
</div>
<div class="section" id="hand-tuning">
<h2><a class="toc-backref" href="#id18">Hand-tuning</a></h2>
<p>Let’s start with an example. Imagine that we want to select the best number of units in the hidden layer (we set up just one hyperparameter for simplicity). The simplest thing is to try different values and select the best one. Let’s say we set up 10 units for the hidden layer and train the network. After the training, we check the accuracy for the validation dataset and it turns out that we classified 65% of the samples correctly.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/hid-units-vs-accuracy-iter1.png"><img alt="Hidden units vs Accuracy, Iteration #1" src="../../../_images/hid-units-vs-accuracy-iter1.png" style="width: 100%;" /></a>
</div>
<p>The accuracy is low, so it’s intuitive to think that we need more units in a hidden layer. Let’s increase the number of units and check the improvement. But, by how many should we increase the number of units? Will small changes make a significant effect on the prediction accuracy? Would it be a good step to set up a number of hidden units equal to 12? Probably not. So let’s go further and explore parameters from the next order of magnitude. We can set up a number of hidden units equal to 100.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/hid-units-vs-accuracy-iter2.png"><img alt="Hidden units vs Accuracy, Iteration #2" src="../../../_images/hid-units-vs-accuracy-iter2.png" style="width: 100%;" /></a>
</div>
<p>For the 100 hidden units, we got prediction accuracy equal to 82% which is a great improvement compared to 65%. Two points in the figure above show us that by increasing number of hidden units we increase the accuracy. We can proceed using the same strategy and train network with 200 hidden units.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/hid-units-vs-accuracy-iter3.png"><img alt="Hidden units vs Accuracy, Iteration #3" src="../../../_images/hid-units-vs-accuracy-iter3.png" style="width: 100%;" /></a>
</div>
<p>After the third iteration, our prediction accuracy is 84%. We’ve increased the number of units by a factor of two and got only 2% of improvement.</p>
<p>We can keep going, but I think judging by this example it is clear that human can select parameters better than Grid search or Random search algorithms. The main reason why is that we are able to learn from our previous mistakes. After each iteration, we memorize and analyze our previous results. This information gives us a much better way for selection of the next set of parameters. And even more than that. The more you work with neural networks the better intuition you develop for what and when to use.</p>
<p>Nevertheless, let’s get back to our optimization problem. How can we automate the process described above? One way of doing this is to apply a Bayesian Optimization.</p>
</div>
<div class="section" id="bayesian-optimization">
<h2><a class="toc-backref" href="#id19">Bayesian Optimization</a></h2>
<p>Bayesian optimization is a derivative-free optimization method. There are a few different algorithm for this type of optimization, but I was specifically interested in Gaussian Process with Acquisition Function. For some people it can resemble the method that we’ve described above in the Hand-tuning section. Gaussian Process uses a set of previously evaluated parameters and resulting accuracy to make an assumption about unobserved parameters. Acquisition Function using this information suggest the next set of parameters.</p>
<div class="section" id="gaussian-process">
<h3><a class="toc-backref" href="#id20">Gaussian Process</a></h3>
<p>The idea behind Gaussian Process is that for every input <span class="math notranslate nohighlight">\(x\)</span> we have output <span class="math notranslate nohighlight">\(y = f(x)\)</span>, where <span class="math notranslate nohighlight">\(f\)</span> is a stochastic function. This function samples output from a gaussian distribution. Also, we can say that each input <span class="math notranslate nohighlight">\(x\)</span> has associated gaussian distribution. Which means that for each input <span class="math notranslate nohighlight">\(x\)</span> gaussian process has defined mean  <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> for some gaussian distribution.</p>
<p>Gaussian Process is a generalization of <a class="reference external" href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">Multivariate Gaussian Distribution</a>. Multivariate Gaussian Distribution is defined by mean vector and covariance matrix, while Gaussian Process is defined by mean function and covariance function. Basically, a function is an infinite vector. Also, we can say that Multivariate Gaussian Distribution is a Gaussian Process for the functions with a discrete number of possible inputs.</p>
<p>I always like to have some picture that shows me a visual description of an algorithm. One of such visualizations of the Gaussian Process I found in the Introduction to Gaussian Process slides <a class="footnote-reference" href="#id7" id="id2">[3]</a>.</p>
<p>Let’s check some Multivariate Gaussian Distribution defined by mean vector <span class="math notranslate nohighlight">\(\mu\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    \mu =
    \left[
    \begin{array}{c}
      0.0 &amp; 1.0 \\
    \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>and covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    \Sigma =
    \left[
    \begin{array}{c}
      1.0 &amp; 0.7 \\
      0.7 &amp; 2.5 \\
    \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>We can sample 100 points from this distribution and make a scatter plot.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/mulvar-gauss-dist-example.png"><img alt="Multivariate Gaussian Distribution Example" src="../../../_images/mulvar-gauss-dist-example.png" style="width: 100%;" /></a>
</div>
<p>Another way to visualize these samples might be <a class="reference external" href="https://en.wikipedia.org/wiki/Parallel_coordinates">Parallel Coordinates</a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/mulvar-gauss-dist-parallel-coords.png"><img alt="Multivariate Gaussian Distribution in Parallel Coordinates Example" src="../../../_images/mulvar-gauss-dist-parallel-coords.png" style="width: 100%;" /></a>
</div>
<p>You should understand that lines that connect points are just an imaginary relations between each coordinate. There is nothing in between Random variable #1 and Random variable #2.</p>
<p>An interesting thing happens when we increase the number of samples.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/mulvar-gauss-dist-parallel-coords-many-samples.png"><img alt="Multivariate Gaussian Distribution in Parallel Coordinates Example with 3000 samples" src="../../../_images/mulvar-gauss-dist-parallel-coords-many-samples.png" style="width: 100%;" /></a>
</div>
<p>Now we can see that lines form a smooth shape. This shape defines a correlation between two random variables. If it’s very narrow in the middle then there is a negative correlation between two random variables.</p>
<p>With scatter plot we are limited to numbers of dimensions that we can visualize, but with Parallel Coordinates we can add more dimensions. Let’s define new Multivariate Gaussian Distribution using 5 random variables.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/mulvar-gauss-dist-parallel-coords-5d.png"><img alt="Multivariate Gaussian Distribution in Parallel Coordinates for multiple dimensions" src="../../../_images/mulvar-gauss-dist-parallel-coords-5d.png" style="width: 100%;" /></a>
</div>
<p>With more variables, it looks more like a function. We can increase the number of dimensions and still be able to visualize Multivariate Gaussian Distribution. The more dimensions we add the more it looks like a set of functions sampled from the Gaussian Process. But in case of Gaussian Process number of dimensions should be infinite.</p>
<p>Let’s get data from the Hand-tuning section (the one where with 10 hidden units we got 65% of accuracy). Using this data we can train Gaussian Process and predict mean and standard deviation for each point <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/gaussian-process-example.png"><img alt="Gaussian Process regression example for second and third iterations" src="../../../_images/gaussian-process-example.png" style="width: 100%;" /></a>
</div>
<p>The blue region defines 95% confidence interval for each point <span class="math notranslate nohighlight">\(x\)</span>. It’s easy to see that the further we go from the observed samples the wider confidence interval becomes which is a logical conclusion. The opposite is true as well. Very similar to the logic that a person uses to select next set of parameters.</p>
<p>From the plot, it looks like observed data points doesn’t have any variance. In fact, the variance is not zero, it’s just really tiny. That’s because our previous Gaussian Process configuration is expecting that our prediction was obtained from a deterministic function which is not true for most neural networks. To fix it we can change the parameter for the Gaussian Process that defines the amount of noise in observed variables. This trick will not only give us a prediction that is less certain but also a mean of the number of hidden units that won’t go through the observed data points.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/gaussian-process-noise-example.png"><img alt="Gaussian Process regression example with noise for second and third iterations" src="../../../_images/gaussian-process-noise-example.png" style="width: 100%;" /></a>
</div>
</div>
<div class="section" id="acquisition-function">
<h3><a class="toc-backref" href="#id21">Acquisition Function</a></h3>
<p>Acquisition Function defines the set of parameter for our next step. There are many different functions <a class="footnote-reference" href="#id5" id="id3">[1]</a> that can help us calculate the best value for the next step. One of the most common is Expected Improvement. There are two ways to compute it. In case if we are trying to find minimum we can use this formula.</p>
<div class="math notranslate nohighlight">
\[g_{min}(x) = max(0, y_{min} - y_{lowest\ expected})\]</div>
<p>where <span class="math notranslate nohighlight">\(y_{min}\)</span> is the minimum observed value <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(y_{lowest\ expected}\)</span> lowest possible value from the confidence interval associated with each possible value <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>In our case, we are trying to find the maximum value. With the small modifications, we can change last formula in the way that will identify Expected Improvement for the maximum value.</p>
<div class="math notranslate nohighlight">
\[g_{max}(x) = max(0, y_{highest\ expected} - y_{max})\]</div>
<p>where <span class="math notranslate nohighlight">\(y_{max}\)</span> is the maximum observed value and <span class="math notranslate nohighlight">\(y_{highest\ expected}\)</span> highest possible value from the confidence interval associated with each possible value <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>Here is an output for each point <span class="math notranslate nohighlight">\(x\)</span> for the Expected Improvement function.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/expected-improvement-example.png"><img alt="Expected Improvement example" src="../../../_images/expected-improvement-example.png" style="width: 100%;" /></a>
</div>
</div>
<div class="section" id="find-number-of-hidden-units">
<h3><a class="toc-backref" href="#id22">Find number of hidden units</a></h3>
<p>Let’s try to build a hyperparameter optimizer based on Gaussian Process regression and Expected Improvement function. We will continue work with the previous problem where we tried to find the best number of hidden units. But for this time we will try to create a network for digit classification tasks.</p>
<p>Let’s define a function that trains the neural network and return prediction error.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span><span class="p">,</span> <span class="n">layers</span>

<span class="k">def</span> <span class="nf">train_network</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    <span class="n">network</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="mi">64</span><span class="p">),</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">Relu</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">),</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
        <span class="p">],</span>

        <span class="c1"># Randomly shuffle dataset before each</span>
        <span class="c1"># training epoch.</span>
        <span class="n">shuffle_data</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>

        <span class="c1"># Do not show training progress in output</span>
        <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>

        <span class="n">step</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
        <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">network</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

    <span class="c1"># Calculates categorical cross-entropy error between</span>
    <span class="c1"># predicted value for x_test and y_test value</span>
    <span class="k">return</span> <span class="n">network</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
<p>Let’s import digits dataset from scikit-learn.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">utils</span>

<span class="n">utils</span><span class="o">.</span><span class="n">reproducible</span><span class="p">()</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">size</span>
<span class="n">n_classes</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># One-hot encoder</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">))</span>
<span class="n">target</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_samples</span><span class="p">),</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span>
<span class="p">)</span>
</pre></div>
</div>
<p>And for the last step, we need to define parameter selection procedure. First, we need to define a function that performs Gaussian Process regression and returns mean and standard deviation of the prediction for the specified input vector.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcess</span>

<span class="k">def</span> <span class="nf">vector_2d</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">array</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">gaussian_process</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">):</span>
    <span class="n">x_train</span> <span class="o">=</span> <span class="n">vector_2d</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">vector_2d</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">x_test</span> <span class="o">=</span> <span class="n">vector_2d</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

    <span class="c1"># Train gaussian process</span>
    <span class="n">gp</span> <span class="o">=</span> <span class="n">GaussianProcess</span><span class="p">(</span><span class="n">corr</span><span class="o">=</span><span class="s1">&#39;squared_exponential&#39;</span><span class="p">,</span>
                         <span class="n">theta0</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">,</span> <span class="n">thetaL</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">thetaU</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">gp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="c1"># Get mean and standard deviation for each possible</span>
    <span class="c1"># number of hidden units</span>
    <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_var</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">eval_MSE</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">y_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">vector_2d</span><span class="p">(</span><span class="n">y_var</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span>
</pre></div>
</div>
<p>Next, we need to apply to the predicted output Expected Improvement (EI) and find out next optimal step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">next_parameter_by_ei</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span><span class="p">,</span> <span class="n">x_choices</span><span class="p">):</span>
    <span class="c1"># Calculate expecte improvement from 95% confidence interval</span>
    <span class="n">expected_improvement</span> <span class="o">=</span> <span class="n">y_min</span> <span class="o">-</span> <span class="p">(</span><span class="n">y_mean</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">y_std</span><span class="p">)</span>
    <span class="n">expected_improvement</span><span class="p">[</span><span class="n">expected_improvement</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">max_index</span> <span class="o">=</span> <span class="n">expected_improvement</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
    <span class="c1"># Select next choice</span>
    <span class="n">next_parameter</span> <span class="o">=</span> <span class="n">x_choices</span><span class="p">[</span><span class="n">max_index</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">next_parameter</span>
</pre></div>
</div>
<p>And finally, we can override all procedure in one function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>

<span class="k">def</span> <span class="nf">hyperparam_selection</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">n_hidden_range</span><span class="p">,</span> <span class="n">func_args</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">func_args</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">func_args</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">min_n_hidden</span><span class="p">,</span> <span class="n">max_n_hidden</span> <span class="o">=</span> <span class="n">n_hidden_range</span>
    <span class="n">n_hidden_choices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">min_n_hidden</span><span class="p">,</span> <span class="n">max_n_hidden</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># To be able to perform gaussian process we need to</span>
    <span class="c1"># have at least 2 samples.</span>
    <span class="n">n_hidden</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">min_n_hidden</span><span class="p">,</span> <span class="n">max_n_hidden</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="o">*</span><span class="n">func_args</span><span class="p">)</span>

    <span class="n">parameters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">)</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

    <span class="n">n_hidden</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">min_n_hidden</span><span class="p">,</span> <span class="n">max_n_hidden</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="o">*</span><span class="n">func_args</span><span class="p">)</span>

        <span class="n">parameters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">)</span>
        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

        <span class="n">y_min</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span> <span class="o">=</span> <span class="n">gaussian_process</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span>
                                         <span class="n">n_hidden_choices</span><span class="p">)</span>

        <span class="n">n_hidden</span> <span class="o">=</span> <span class="n">next_parameter_by_ei</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span><span class="p">,</span>
                                        <span class="n">n_hidden_choices</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">y_min</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">n_hidden</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
            <span class="c1"># Lowest expected improvement value have been achieved</span>
            <span class="k">break</span>

    <span class="n">min_score_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">parameters</span><span class="p">[</span><span class="n">min_score_index</span><span class="p">]</span>
</pre></div>
</div>
<p>Now we are able to run a few iterations and find a number of hidden units that gave better results during the training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">best_n_hidden</span> <span class="o">=</span> <span class="n">hyperparam_selection</span><span class="p">(</span>
    <span class="n">train_network</span><span class="p">,</span>
    <span class="n">n_hidden_range</span><span class="o">=</span><span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1000</span><span class="p">],</span>
    <span class="n">func_args</span><span class="o">=</span><span class="p">[</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">],</span>
    <span class="n">n_iter</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/hyperparam-selection-nn-hidden-units.png"><img alt="Select number of hidden units for Neural Network" src="../../../_images/hyperparam-selection-nn-hidden-units.png" style="width: 100%;" /></a>
</div>
<p>With small modifications, it’s possible to add an additional functionality to the function that allows optimizing more hyperparameters at once.</p>
</div>
<div class="section" id="disadvantages-of-gp-with-ei">
<h3><a class="toc-backref" href="#id23">Disadvantages of GP with EI</a></h3>
<p>There are a few disadvantages related to the Gaussian Process with Expected Improvement.</p>
<ol class="arabic simple">
<li>It doesn’t work well for categorical variables. In case if neural networks it can be a type of activation function.</li>
<li>GP with EI selects new set of parameters based on the best observation. Neural Network usually involves randomization (like weight initialization and dropout) during the training process which influences a final score. Running neural network with the same parameters can lead to different scores. Which means that our best score can be just lucky output for the specific set of parameters.</li>
<li>It can be difficult to select right hyperparameters for Gaussian Process. Gaussian Process has lots of different kernel types. In addition you can construct more complicated kernels using simple kernels as a building block.</li>
<li>It works slower when number of hyperparameters increases. That’s an issue when you deal with a huge number of parameters.</li>
</ol>
</div>
</div>
<div class="section" id="tree-structured-parzen-estimators-tpe">
<h2><a class="toc-backref" href="#id24">Tree-structured Parzen Estimators (TPE)</a></h2>
<div class="section" id="overview">
<h3><a class="toc-backref" href="#id25">Overview</a></h3>
<p>Tree-structured Parzen Estimators (TPE) fixes disadvantages of the Gaussian Process. Each iteration TPE collects new observation and at the end of the iteration, the algorithm decides which set of parameters it should try next. The main idea is similar, but an algorithm is completely different</p>
<p>At the very beginning, we need to define a prior distribution for out hyperparameters. By default, they can be all uniformly distributed, but it’s possible to associate any hyperparameter with some random unimodal distribution.</p>
<p>For the first few iterations, we need to warn up TPE algorithm. It means that we need to collect some data at first before we can apply TPE. The best and simplest way to do it is just to perform a few iterations of Random Search. A number of iterations for Random Search is a parameter defined by the user for the TPE algorithm.</p>
<p>When we collected some data we can finally apply TPE. The next step is to divide collected observations into two groups. The first group contains observations that gave best scores after evaluation and the second one - all other observations. And the goal is to find a set of parameters that more likely to be in the first group and less likely to be in the second group. The fraction of the best observations is defined by the user as a parameter for the TPE algorithm. Typically, it’s 10-25% of observations.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/tpe-observation-groups.png"><img alt="Observation groups for TPE" src="../../../_images/tpe-observation-groups.png" style="width: 100%;" /></a>
</div>
<p>As you can see we are no longer rely on the best observation. Instead, we use a distribution of the best observations. The more iterations we use during the Random Search the better distribution we have at the beginning.</p>
<p>The next part of the TPE is to model likelihood probability for each of the two groups. This is the next big difference between Gaussian Process and TPE. For Gaussian Process we’ve modeled posterior probability instead of likelihood probability. Using the likelihood probability from the first group (the one that contains best observations) we sample the bunch of candidates. From the sampled candidates we try to find a candidate that more likely to be in the first group and less likely to be in the second one. The following formula defines Expected Improvement per each candidate.</p>
<div class="math notranslate nohighlight">
\[EI(x) = \frac{l(x)}{g(x)}\]</div>
<p>Where <span class="math notranslate nohighlight">\(l(x)\)</span> is a probability being in the first group and <span class="math notranslate nohighlight">\(g(x)\)</span> is a probability being in the second group.</p>
<p>Here is an example. Let’s say we have predefined distribution for both groups. From the group #1, we sample 6 candidates. And for each, we calculate Expected Improvement. A parameter that has the highest improvement is the one that we will use for the next iteration.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/tpe-sampled-candidates.png"><img alt="Candidates sampling for TPE" src="../../../_images/tpe-sampled-candidates.png" style="width: 100%;" /></a>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/tpe-expected-improvement.png"><img alt="Expected improvement for TPE" src="../../../_images/tpe-expected-improvement.png" style="width: 100%;" /></a>
</div>
<p>In the example, I’ve used t-distributions, but in TPE distribution models using <a class="reference external" href="https://www.cs.utah.edu/~suyash/Dissertation_html/node11.html">parzen-window density estimators</a>. The main idea is that each sample defines gaussian distribution with specified mean (value of the hyperparameter) and standard deviation. Then all these points stacks together and normalized to assure that output is Probability Density Function (PDF). That’s why <cite>Parzen estimators</cite> appears in the name of the algorithm.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../../../_images/parzen-estimators.png"><img alt="Parzen estimators" src="../../../_images/parzen-estimators.png" style="width: 100%;" /></a>
</div>
<p>And the <cite>tree-structured</cite> means that parameter space defines in a form of a tree. Later we will try to find the best number of layers for the network. In our case, we will try to decide whether it’s better to use one or two hidden layers. In case if we use two hidden layers we should define the number of hidden units for the first and second layer independently. If we use one hidden layer we don’t need to define the number of hidden units for the second hidden layer, because it doesn’t exist for the specified set of parameter. Basically, it means that a number of hidden units in the second hidden layer depends on the number of hidden layers. Which means that parameters have tree-structured dependencies.</p>
</div>
<div class="section" id="hyperparameter-optimization-for-mnist-dataset">
<h3><a class="toc-backref" href="#id26">Hyperparameter optimization for MNIST dataset</a></h3>
<p>Let’s make an example. We’re going to use MNIST dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_openml</span><span class="p">(</span><span class="s1">&#39;mnist_784&#39;</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">target_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">target_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">X</span> <span class="o">/=</span> <span class="mf">255.</span>
<span class="n">X</span> <span class="o">-=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="n">test_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mf">7.</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>For hyperparameter selection, I’m going to use <a class="reference external" href="https://github.com/hyperopt/hyperopt">hyperopt</a> library. It has implemented TPE algorithm.</p>
<p>The hyperopt library gives the ability to define a prior distribution for each parameter. In the table below you can find information about parameters that we are going to tune.</p>
<table border="1" class="docutils">
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Parameter name</th>
<th class="head">Distribution</th>
<th class="head">Values</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Step size</td>
<td>Log-uniform</td>
<td><span class="math notranslate nohighlight">\(x \in [0.01, 0.5]\)</span></td>
</tr>
<tr class="row-odd"><td>Batch size</td>
<td>Log-uniform integer</td>
<td><span class="math notranslate nohighlight">\(x \in [16, 512]\)</span></td>
</tr>
<tr class="row-even"><td>Activation function</td>
<td>Categorical</td>
<td><span class="math notranslate nohighlight">\(x \in \{Relu, PRelu, Elu, Sigmoid, Tanh\}\)</span></td>
</tr>
<tr class="row-odd"><td>Number of hidden layers</td>
<td>Categorical</td>
<td><span class="math notranslate nohighlight">\(x \in \{1, 2\}\)</span></td>
</tr>
<tr class="row-even"><td>Number of units in the first layer</td>
<td>Uniform integer</td>
<td><span class="math notranslate nohighlight">\(x \in [50, 1000]\)</span></td>
</tr>
<tr class="row-odd"><td>Number of units in the second layer (In case if it defined)</td>
<td>Uniform integer</td>
<td><span class="math notranslate nohighlight">\(x \in [50, 1000]\)</span></td>
</tr>
<tr class="row-even"><td>Dropout layer</td>
<td>Uniform</td>
<td><span class="math notranslate nohighlight">\(x \in [0, 0.5]\)</span></td>
</tr>
</tbody>
</table>
<p>Here is one way to define our parameters in hyperopt.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">hyperopt</span> <span class="kn">import</span> <span class="n">hp</span>

<span class="k">def</span> <span class="nf">uniform_int</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">):</span>
    <span class="c1"># `quniform` returns:</span>
    <span class="c1"># round(uniform(low, high) / q) * q</span>
    <span class="k">return</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">loguniform_int</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">):</span>
    <span class="c1"># Do not forget to make a logarithm for the</span>
    <span class="c1"># lower and upper bounds.</span>
    <span class="k">return</span> <span class="n">hp</span><span class="o">.</span><span class="n">qloguniform</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lower</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">upper</span><span class="p">),</span> <span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">parameter_space</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;step&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="s1">&#39;step&#39;</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
    <span class="s1">&#39;layers&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;layers&#39;</span><span class="p">,</span> <span class="p">[{</span>
        <span class="s1">&#39;n_layers&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s1">&#39;n_units_layer&#39;</span><span class="p">:</span> <span class="p">[</span>
            <span class="n">uniform_int</span><span class="p">(</span><span class="s1">&#39;n_units_layer_11&#39;</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span>
        <span class="p">],</span>
    <span class="p">},</span> <span class="p">{</span>
        <span class="s1">&#39;n_layers&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="s1">&#39;n_units_layer&#39;</span><span class="p">:</span> <span class="p">[</span>
            <span class="n">uniform_int</span><span class="p">(</span><span class="s1">&#39;n_units_layer_21&#39;</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span>
            <span class="n">uniform_int</span><span class="p">(</span><span class="s1">&#39;n_units_layer_22&#39;</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span>
        <span class="p">],</span>
    <span class="p">}]),</span>
    <span class="s1">&#39;act_func_type&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;act_func_type&#39;</span><span class="p">,</span> <span class="p">[</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Relu</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">PRelu</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Elu</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Tanh</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Sigmoid</span>
    <span class="p">]),</span>

    <span class="s1">&#39;dropout&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
    <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="n">loguniform_int</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
<span class="p">}</span>
</pre></div>
</div>
<p>I won’t get into details. I think that definitions are pretty clear from the code. In case if you want to learn more about hyperopt parameter space initialization you can check <a class="reference external" href="https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions">this link</a>.</p>
<p>Next we need to construct a function that we want to minimize. In our case function should train network using training dataset and return cross entropy error for validation dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="k">def</span> <span class="nf">train_network</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Parameters:&quot;</span><span class="p">)</span>
    <span class="n">pprint</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>
</pre></div>
</div>
<p>First of all, in the training function, we need to extract our parameter.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">step</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">])</span>
<span class="n">proba</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;dropout&#39;</span><span class="p">]</span>
<span class="n">activation_layer</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;act_func_type&#39;</span><span class="p">]</span>
<span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;layers&#39;</span><span class="p">][</span><span class="s1">&#39;n_units_layer&#39;</span><span class="p">]]</span>
</pre></div>
</div>
<p>Note that some of the parameters I converted to the integer. The problem is that hyperopt returns float types and we need to convert them.</p>
<p>Next, we need to construct network based on the presented information. In our case, we use only one or two hidden layers, but it can be any arbitrary number of layers.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="mi">784</span><span class="p">)</span>

<span class="k">for</span> <span class="n">layer_size</span> <span class="ow">in</span> <span class="n">layer_sizes</span><span class="p">:</span>
    <span class="n">network</span> <span class="o">=</span> <span class="n">network</span> <span class="o">&gt;&gt;</span> <span class="n">activation_layer</span><span class="p">(</span><span class="n">layer_size</span><span class="p">)</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">network</span> <span class="o">&gt;&gt;</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">proba</span><span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="n">layers</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>To learn more about layers in NeuPy you should check <a class="reference internal" href="../../../docs/layers/basics.html#layers-basics"><span class="std std-ref">documentation</span></a>.</p>
<p>After that, we can define training algorithm for the network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span>
<span class="kn">from</span> <span class="nn">neupy.exceptions</span> <span class="kn">import</span> <span class="n">StopTraining</span>

<span class="k">def</span> <span class="nf">on_epoch_end</span><span class="p">(</span><span class="n">network</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">network</span><span class="o">.</span><span class="n">training_errors</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">StopTraining</span><span class="p">(</span><span class="s2">&quot;Training was interrupted. Error is to high.&quot;</span><span class="p">)</span>

<span class="n">mnet</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">RMSProp</span><span class="p">(</span>
    <span class="n">network</span><span class="p">,</span>

    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">,</span>

    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
    <span class="n">shuffle_data</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>

    <span class="n">signals</span><span class="o">=</span><span class="n">on_epoch_end</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>All settings should be clear from the code. You can check <a class="reference internal" href="../../../apidocs/neupy.algorithms.gd.rmsprop.html#neupy.algorithms.gd.rmsprop.RMSProp" title="neupy.algorithms.gd.rmsprop.RMSProp"><span class="xref py py-class docutils literal"><span class="pre">RMSProp</span></span></a> documentation to find more information about its input parameters. In addition, I’ve added a simple rule that interrupts training when the error is too high. This is an example of a simple rule that can be changed.</p>
<p>Now we can train our network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mnet</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
<p>And at the end of the function, we can check some information about the training progress.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">score</span> <span class="o">=</span> <span class="n">mnet</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">mnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">y_predicted</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Final score: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: {:.2%}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>

<span class="k">return</span> <span class="n">score</span>
</pre></div>
</div>
<p>You can see that I’ve used two evaluation metrics. First one is cross-entropy. NeuPy uses it as a validation error function when we call the <span class="docutils literal"><span class="pre">score</span></span> method. The second one is just a prediction accuracy.</p>
<p>And finally, we run hyperparameter optimization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">hyperopt</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="c1"># Object stores all information about each trial.</span>
<span class="c1"># Also, it stores information about the best trial.</span>
<span class="n">trials</span> <span class="o">=</span> <span class="n">hyperopt</span><span class="o">.</span><span class="n">Trials</span><span class="p">()</span>

<span class="n">tpe</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
    <span class="n">hyperopt</span><span class="o">.</span><span class="n">tpe</span><span class="o">.</span><span class="n">suggest</span><span class="p">,</span>

    <span class="c1"># Sample 1000 candidate and select candidate that</span>
    <span class="c1"># has highest Expected Improvement (EI)</span>
    <span class="n">n_EI_candidates</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>

    <span class="c1"># Use 20% of best observations to estimate next</span>
    <span class="c1"># set of parameters</span>
    <span class="n">gamma</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>

    <span class="c1"># First 20 trials are going to be random</span>
    <span class="n">n_startup_jobs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">hyperopt</span><span class="o">.</span><span class="n">fmin</span><span class="p">(</span>
    <span class="n">train_network</span><span class="p">,</span>

    <span class="n">trials</span><span class="o">=</span><span class="n">trials</span><span class="p">,</span>
    <span class="n">space</span><span class="o">=</span><span class="n">parameter_space</span><span class="p">,</span>

    <span class="c1"># Set up TPE for hyperparameter optimization</span>
    <span class="n">algo</span><span class="o">=</span><span class="n">tpe</span><span class="p">,</span>

    <span class="c1"># Maximum number of iterations. Basically it trains at</span>
    <span class="c1"># most 200 networks before selecting the best one.</span>
    <span class="n">max_evals</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>And after all trials, we can check the best one in the <span class="docutils literal"><span class="pre">trials.best_trial</span></span> attribute.</p>
</div>
<div class="section" id="disadvantages-of-tpe">
<h3><a class="toc-backref" href="#id27">Disadvantages of TPE</a></h3>
<p>On of the biggest disadvantages of this algorithm is that it selects parameters independently from each other. For instance, there is a clear relation between regularization and number of training epoch parameters. With regularization, we usually can train network for more epochs and with more epochs we can achieve better results. On the other hand without regularization, many epochs can be a bad choice because network starts overfitting and validation error increases. Without taking into account the state of the regularization variable each next choice for the number of epochs can look arbitrary.</p>
<p>It’s good in case if you now that some variables have relations. To overcome problem from the previous example you can construct two different choices for epochs. The first one will enable regularization and selects a number of epochs from the <span class="math notranslate nohighlight">\([500, 1000]\)</span> range. And the second one without regularization and selects number of epochs from the <span class="math notranslate nohighlight">\([10, 200]\)</span> range.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;training_parameters&#39;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s1">&#39;regularization&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
        <span class="s1">&#39;n_epochs&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">&#39;n_epochs&#39;</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">},</span> <span class="p">{</span>
        <span class="s1">&#39;regularization&#39;</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
        <span class="s1">&#39;n_epochs&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">&#39;n_epochs&#39;</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">},</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="summary">
<h2><a class="toc-backref" href="#id28">Summary</a></h2>
<p>The Bayesian Optimization and TPE algorithms show great improvement over the classic hyperparameter optimization methods. They allow to learn from the training history and give better and better estimations for the next set of parameters. But it still takes lots of time to apply these algorithms. It’s great if you have an access to multiple machines and you can parallel parameter tuning procedure <a class="footnote-reference" href="#id8" id="id4">[4]</a>, but usually, it’s not an option. Sometimes it’s better just to avoid hyperparameter optimization. In case if you just try to build a network for trivial problems like image classification it’s better to use existed architectures with pre-trained parameters like <a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/cnn/vgg19.py">VGG19</a> or <a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/cnn/resnet50.py">ResNet</a>.</p>
<p>For unique problems that don’t have pre-trained networks the classic and simple hand-tuning is a great way to start. A few iterations can give you a good architecture which won’t be the state-of-the-art but should give you satisfying result with a minimum of problems. In case if accuracy does not suffice your needs you can always boost your performance getting more data or developing ensembles with different models.</p>
</div>
<div class="section" id="source-code">
<h2><a class="toc-backref" href="#id29">Source Code</a></h2>
<p>All source code is available on GitHub in the <a class="reference external" href="https://github.com/itdxer/neupy/blob/master/notebooks/Hyperparameter%20optimization%20for%20Neural%20Networks.ipynb">iPython notebook</a>. It includes all visualizations and hyperparameter selection algorithms.</p>
</div>
<div class="section" id="references">
<h2><a class="toc-backref" href="#id30">References</a></h2>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[1]</a></td><td>Bayesian Optimization and Acquisition Functions from <a class="reference external" href="http://www.cse.wustl.edu/~garnett/cse515t/files/lecture_notes/12.pdf">http://www.cse.wustl.edu/~garnett/cse515t/files/lecture_notes/12.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Gaussian Processes in Machine Learning from <a class="reference external" href="http://mlg.eng.cam.ac.uk/pub/pdf/Ras04.pdf">http://mlg.eng.cam.ac.uk/pub/pdf/Ras04.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[3]</a></td><td>Slides: Introduction to Gaussian Process from <a class="reference external" href="https://www.cs.toronto.edu/~hinton/csc2515/notes/gp_slides_fall08.pdf">https://www.cs.toronto.edu/~hinton/csc2515/notes/gp_slides_fall08.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id8" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[4]</a></td><td>Preliminary Evaluation of Hyperopt Algorithms on HPOLib from <a class="reference external" href="http://compneuro.uwaterloo.ca/files/publications/bergstra.2014.pdf">http://compneuro.uwaterloo.ca/files/publications/bergstra.2014.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id9" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[5]</td><td>Algorithms for Hyper-Parameter Optimization from <a class="reference external" href="http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf">http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id10" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[6]</td><td>Slides: Pattern Recognition, Lecture 6 from <a class="reference external" href="http://www.csd.uwo.ca/~olga/Courses/CS434a_541a/Lecture6.pdf">http://www.csd.uwo.ca/~olga/Courses/CS434a_541a/Lecture6.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id11" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[7]</a></td><td>Low-discrepancy sampling methods from <a class="reference external" href="http://planning.cs.uiuc.edu/node210.html">http://planning.cs.uiuc.edu/node210.html</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id12" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[8]</td><td>Parzen-Window Density Estimation from <a class="reference external" href="https://www.cs.utah.edu/~suyash/Dissertation_html/node11.html">https://www.cs.utah.edu/~suyash/Dissertation_html/node11.html</a></td></tr>
</tbody>
</table>
</div>
</div>

    <div class="postmeta">
        <div class="author">
            <span>Posted by Yurii Shevchuk</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="../../../tags/visualization.html">visualization</a>, <a href="../../../tags/backpropagation.html">backpropagation</a>, <a href="../../../tags/supervised.html">supervised</a>, <a href="../../../tags/hyperparameter_optimization.html">hyperparameter optimization</a></span>
        </div>
        </div><ul class="related clearfix">
            <li class="left"> &laquo; <a href="../../../2017/12/09/sofm_applications.html">Self-Organizing Maps and Applications</a></li>
            <li class="right"><a href="../../11/12/mnist_classification.html">Image classification, MNIST digits</a> &raquo; </li>
        </ul><div id="disqus_thread"></div><script type="text/javascript">    var disqus_shortname = "neupy";    var disqus_identifier = "2016/12/17/hyperparameter_optimization_for_neural_networks";    disqus_thread();</script><noscript>Please enable JavaScript to view the    <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></article><aside class="sidebar"><section><div class="widget" id="searchbox" role="search">
    <h1><a href="#searchbox">Search</a></h1>
    <form action="../../../search.html" method="get">
          <div class="box">
            <div class="search-input-container">
                <span class="icon"><i class="fa fa-search"></i></span>
                <input type="search" name="q" id="search" placeholder="Search..." />
            </div>
          </div>
    </form>
</div></section><section><div class="widget">
    <h1>Install NeuPy</h1>
    <div class="highligh-bash">
        <div class="highlight">
            <pre>pip install neupy</pre>
        </div>
    </div>
    <p>
        <div>Learn more about NeuPy reading <a href="../../../docs/tutorials.html">tutorials</a> and <a href="../../../pages/documentation.html">documentation</a>.</div>
    </p>
</div></section><section><div class="widget">
    <h1>Issues and feature requests</h1>
    <p>
        If you find a bug or want to suggest a new feature feel free to
        <a href="https://github.com/itdxer/neupy/issues/new">create an issue</a>
        on Github
    </p>
</div></section></aside></div> <!-- #main --></div> <!-- #main-container -->

        <div class="footer-container" role="contentinfo"><footer class="wrapper">&copy; Copyright 2015 - 2019, Yurii Shevchuk. Powered by <a href="http://www.tinkerer.me/">Tinkerer</a> and <a href="http://sphinx.pocoo.org/">Sphinx</a>.</footer></div> <!-- footer-container -->

      </div> <!--! end of #container --><!--[if lt IE 7 ]>
          <script src="//ajax.googleapis.com/ajax/libs/chrome-frame/1.0.3/CFInstall.min.js"></script>
          <script>window.attachEvent('onload',function(){CFInstall.check({mode:'overlay'})})</script>
        <![endif]-->
    </body>
</html>